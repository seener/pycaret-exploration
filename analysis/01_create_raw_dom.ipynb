{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "k-pce",
   "display_name": "k-pce"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as db\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the sql engine for postgres db\n",
    "def sql_build_connection(dbname):\n",
    "    '''A simple function to derive a postgresql connection.\n",
    "\n",
    "    Summary\n",
    "    -------\n",
    "    Create a postgresql connection. This assumes that there is a file\n",
    "    called .sql in the user root folder with database credentials.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    engine: sql engine\n",
    "        A postgresql engine object\n",
    "\n",
    "    '''\n",
    "    root = os.path.expanduser('~')\n",
    "    with open(f'{root}/.sqluser', 'r') as f:\n",
    "        creds = json.load(f)\n",
    "    connection = (f'''postgresql://{creds[\"uid\"]}:{creds[\"pwd\"]}@localhost:5432/{dbname}''')\n",
    "    engine = db.connect(connection)\n",
    "    return engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sql_table_create_parts(file_, prefix=''):\n",
    "    '''Create formated sql string for column definition in table create statement.\n",
    "\n",
    "    Summary\n",
    "    -------\n",
    "    Based on the metadata provided in file_ the list of columns with the proper definitions as well as generates the table name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_: string\n",
    "        A file path to a meta json object\n",
    "    prefix: string\n",
    "        Optional: the prefix to be used for the table naming.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    source_path: string\n",
    "        The path for the source data\n",
    "    sql_name: string\n",
    "        The name of the sql table\n",
    "    sql_columns: string\n",
    "        The sql syntax for defining the columns for the table\n",
    "    '''\n",
    "    # get metadata\n",
    "    with open(file_, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    # build table and column elements\n",
    "    source_path = meta['source']\n",
    "    table_name = source_path.split('/')[-1][:-4]\n",
    "    column_names = list(meta['data_type'].keys())\n",
    "    data_types = list(meta['data_type'].values())\n",
    "    has_null = list(meta['na_count'].values())\n",
    "    max_length = list(meta['char_max_len'].values())\n",
    "\n",
    "    # add prefix to sql table name\n",
    "    sql_table = f'''{prefix}{table_name}'''\n",
    "\n",
    "    # correct data types to sql types\n",
    "    replacements = {'int64': 'integer', 'float64': 'double precision', 'object': 'character varying'}\n",
    "    sql_types = []\n",
    "    for t in data_types:\n",
    "        sql_types.append(replacements.get(t))\n",
    "\n",
    "    # set the max string length\n",
    "    sql_str_len = [round(v + 15, -1) for v in max_length]\n",
    "\n",
    "    # convert has null to boolean\n",
    "    sql_not_null = [not bool(v) for v in has_null]\n",
    "\n",
    "    # build string with column syntax\n",
    "    sql_columns = False\n",
    "    for i, column in enumerate(column_names):\n",
    "        dtype = sql_types[i]\n",
    "        \n",
    "        # if the data type is a character then add\n",
    "        if dtype[:4] == 'char':\n",
    "            size = int(sql_str_len[i])\n",
    "            dtype = f'''{dtype}({size})'''\n",
    "        not_null = sql_not_null[i]\n",
    "        \n",
    "        # add not_null qualifier where true\n",
    "        if not_null:\n",
    "            dtype = f'''{dtype} NOT NULL'''\n",
    "        \n",
    "        # build the sql string for column defitions\n",
    "        if sql_columns:\n",
    "            sql_columns = f'''{sql_columns} \"{column}\" {dtype},'''\n",
    "        else:\n",
    "            sql_columns = f'''\"{column}\" {dtype},'''\n",
    "\n",
    "    sql_columns = sql_columns[:-1]\n",
    "    return source_path, sql_table, sql_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_create_table(table, columns, connect, index_cols=False):\n",
    "    '''Build a sql table based on name and column specs provided.\n",
    "\n",
    "    Summary\n",
    "    -------\n",
    "    Create a table in the sql database. If the table exists it is droped and replaced with the new definition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table: string\n",
    "        The name of the table to be created\n",
    "    columns: string\n",
    "        The string containing the column definitions for the table\n",
    "    conncet: database connection to be used\n",
    "        Database engine to be used\n",
    "    index_cols: string\n",
    "        Optional parameter that should be a comma seperated list of column names for construction of the index\n",
    "    '''    \n",
    "    # sql to drop table if exists and create table based on definition provided\n",
    "    drop = (f'''DROP TABLE IF EXISTS \"{table}\";''')\n",
    "    create = (f'''CREATE TABLE \"{table}\" ({columns});''')\n",
    "    if index_cols:\n",
    "        index = (f''' CREATE INDEX idx_{table} ON {table} ({index_cols})''')\n",
    "    \n",
    "    # build the cursor\n",
    "    cursor = connect.cursor()\n",
    "\n",
    "    # execute statements\n",
    "    cursor.execute(drop)\n",
    "    cursor.execute(create)\n",
    "    if index_cols:\n",
    "        cursor.execute(index)\n",
    "    \n",
    "    # commit statements\n",
    "    connect.commit()\n",
    "    cursor.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_csv_loader(csv_path, target_table, connect):\n",
    "    '''Load csv data into database using pandas and sqlalchemy.\n",
    "\n",
    "    Summary\n",
    "    -------\n",
    "    Take the file path to a csv dataset, load the csv into pandas dataframe, then pipe into database using sqlachemy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path: string\n",
    "        The Fully qualify path to the csv data to be loaded into sql\n",
    "    target_table: mapper\n",
    "        The table object for the target table in the database\n",
    "    conncet: database connection to be used\n",
    "        Database engine to be used\n",
    "    '''\n",
    "    # build the cursor object\n",
    "    cursor = connect.cursor()\n",
    "\n",
    "    # run the data load\n",
    "    copy_cmd = f'''COPY {target_table} FROM STDIN WITH (FORMAT CSV, HEADER TRUE, DELIMITER ',', FORCE_NULL(\"VALUE\"))'''\n",
    "    truncate = f'''TRUNCATE TABLE {target_table}'''\n",
    "    with open(csv_path, 'r') as file_:\n",
    "        cursor.execute(truncate)\n",
    "        cursor.copy_expert(sql=copy_cmd, file=file_)\n",
    "        connect.commit()\n",
    "        cursor.close()\n",
    "    print (f'The data from {csv_path} has been written to {target_table}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sql connection\n",
    "connection = sql_build_connection('analytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metadata to be used for building the tables\n",
    "meta_dir = os.path.realpath('../data')\n",
    "files = os.listdir(meta_dir)\n",
    "match = 'meta.json'\n",
    "meta_json = [f'{meta_dir}/{keep}' for keep in files if keep.endswith(match)]\n",
    "\n",
    "# mappings to make sure that things get labelled correctly\n",
    "prefixes = {\n",
    "    '/Users/sean/Projects/pycaret-exploration/data/14100090-meta.json': 'labourforce_', \n",
    "    '/Users/sean/Projects/pycaret-exploration/data/14100092-meta.json': 'emp_naics_', \n",
    "    '/Users/sean/Projects/pycaret-exploration/data/14100312-meta.json': 'emp_nocs_'\n",
    "    }\n",
    "\n",
    "for_index = {\n",
    "    '/Users/sean/Projects/pycaret-exploration/data/14100090-meta.json': 'Labour force characteristics', \n",
    "    '/Users/sean/Projects/pycaret-exploration/data/14100092-meta.json': 'North American Industry Classification System (NAICS)',\n",
    "    '/Users/sean/Projects/pycaret-exploration/data/14100312-meta.json': 'National Occupational Classification (NOC)'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for path in meta_json:\n",
    "    # the parts for building the sql table\n",
    "    source, table, columns = sql_table_create_parts(path, prefixes[path])\n",
    "    index = f'''\"REF_DATE\", \"GEO\", \"{for_index[path]}\"'''\n",
    "\n",
    "    # create the sql table\n",
    "    sql_create_table(table, columns, connection, index)\n",
    "\n",
    "    # load the data into sql\n",
    "    sql_csv_loader(source, table, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the database connection\n",
    "connection.close()"
   ]
  }
 ]
}